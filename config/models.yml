#zephyr:
#  # Change this value based on your model and your GPU VRAM pool.
#  n_gpu_layers: 16
#  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
#  n_batch: 2048
#  temperature: 0
#  model_name: 'zephyr-7b-beta.Q5_K_M.gguf'
#  n_ctx: 4096
#  max_tokens: 4096
#  top_p: 0.95
#  top_k: 20
#neuralhermes:
#  # Change this value based on your model and your GPU VRAM pool.
#  n_gpu_layers: 16
#  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
#  n_batch: 2048
#  temperature: 0
#  model_name: 'neuralhermes-2.5-mistral-7b.Q5_K_M.gguf'
#  n_ctx: 4096
#  max_tokens: 4096
#  top_p: 0.95
#  top_k: 20
memory_model:
  # Change this value based on your model and your GPU VRAM pool.
  n_gpu_layers: 8
  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
  n_batch: 512
  temperature: 0
  model_name: 'neuralhermes-2.5-mistral-7b.Q5_K_M.gguf'
  n_ctx: 2048
  max_tokens: 512
  top_p: 0.95
  top_k: 20
conversational_model:
  # Change this value based on your model and your GPU VRAM pool.
  n_gpu_layers: 16
  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
  n_batch: 2048
  temperature: 0.15
  model_name: 'neuralhermes-2.5-mistral-7b.Q5_K_M.gguf'
  n_ctx: 4096
  max_tokens: 4096
  top_p: 0.95
  top_k: 20
api_model:
  # Change this value based on your model and your GPU VRAM pool.
  n_gpu_layers: 16
  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
  n_batch: 1028
  temperature: 0
  model_name: 'neuralhermes-2.5-mistral-7b.Q5_K_M.gguf'
  n_ctx: 2048
  max_tokens: 2048
  top_p: 0.95
  top_k: 20
classification_model:
  # Change this value based on your model and your GPU VRAM pool.
  n_gpu_layers: 8
  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.
  n_batch: 512
  temperature: 0
  model_name: 'neuralhermes-2.5-mistral-7b.Q5_K_M.gguf'
  n_ctx: 2048
  max_tokens: 2
  top_p: 0.95
  top_k: 20
